{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f865e1",
   "metadata": {},
   "source": [
    "# **Multi-Layer Perceptron (MLP) Classification**\n",
    "\n",
    "## **Project Overview:**\n",
    "\n",
    "This notebook implements a Multi-Layer Perceptron (MLP) neural network for a real-world \n",
    "binary classification task. We will handle all aspects of the machine learning pipeline \n",
    "including data preparation, model implementation, training, and evaluation.\n",
    "\n",
    "Authors: Rodrigo Medeiros, Matheus Castellucci e João Pedro Rodrigues "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c41b4",
   "metadata": {},
   "source": [
    "## **Dataset Selection**\n",
    "\n",
    "### **Binary Classification with a Bank Dataset**\n",
    "\n",
    "**Dataset:** [Binary Classification with a Bank Dataset](https://www.kaggle.com/competitions/playground-series-s5e8)\n",
    "\n",
    "This dataset comes from Kaggle's Playground Series, which provides synthetic datasets \n",
    "generated from real-world data to allow practitioners to explore machine learning \n",
    "techniques in a competition-style format.\n",
    "\n",
    "The dataset focuses on a binary classification problem related to banking data. The goal is to predict whether a client will subscribe to a bank term deposit.\n",
    "\n",
    "**Why this dataset:**\n",
    "\n",
    "- Tabular bank dataset suitable for an MLP (mix of categorical and numerical features).\n",
    "- Good practice for preprocessing (categorical encoding, scaling), class imbalance checks, feature engineering, and standard model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c441c9c",
   "metadata": {},
   "source": [
    "## **Dataset Explanation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64968035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2475ae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "train_path = DATA_DIR / \"train.csv\"\n",
    "test_path = DATA_DIR / \"test.csv\"\n",
    "target_col = 'y'\n",
    "id_col = 'id'\n",
    "\n",
    "# Load train\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "print(\"\\nTrain shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "\n",
    "# Print columns and types\n",
    "numerical_features = train.select_dtypes(include=['number']).columns.tolist()\n",
    "if id_col in numerical_features:\n",
    "    numerical_features.remove(id_col)\n",
    "if target_col in numerical_features:\n",
    "    numerical_features.remove(target_col)\n",
    "categorical_features = train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"\\nNumerical features:\", numerical_features)\n",
    "print(\"Categorical features:\", categorical_features)\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(train.head())\n",
    "\n",
    "# Basic checks\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(train.isna().sum())\n",
    "\n",
    "# Target distribution\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(train[target_col].value_counts())\n",
    "print(\"\\nTarget proportion (positive):\", train[target_col].value_counts(normalize=True).get(1, None) or train[target_col].value_counts(normalize=True).get('yes', None) or \"Unknown format\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0aa5ae",
   "metadata": {},
   "source": [
    "### **Dataset overview**\n",
    "\n",
    "| Property | Value |\n",
    "|-----------|-------|\n",
    "| **Samples (train)** | ≈ 750 000 rows |\n",
    "| **Features** | 17 columns (16 predictors + 1 target) |\n",
    "| **Target** | `y` (binary: 0 = no deposit, 1 = deposit) |\n",
    "| **Positive class proportion** | ≈ 12 % |\n",
    "| **Missing values** | None |\n",
    "\n",
    "### **Feature types**\n",
    "\n",
    "| Category | Columns |\n",
    "|-----------|----------|\n",
    "| **Numeric** | `age`, `balance`, `day`, `duration`, `campaign`, `pdays`, `previous` |\n",
    "| **Categorical** | `job`, `marital`, `education`, `default`, `housing`, `loan`, `contact`, `month`, `poutcome` |\n",
    "| **ID column** | `id` (unique identifier, to be excluded from training) |\n",
    "\n",
    "### **Domain Knowledge**\n",
    "\n",
    "- `age`: Age of the client (numeric)\n",
    "\n",
    "- `job`: Type of job (categorical: \"admin.\", \"blue-collar\", \"entrepreneur\", etc.)\n",
    "\n",
    "- `marital`: Marital status (categorical: \"married\", \"single\", \"divorced\")\n",
    "education: Level of education (categorical: \"primary\", \"secondary\", \"tertiary\", \"unknown\")\n",
    "\n",
    "- `default`: Has credit in default? (categorical: \"yes\", \"no\")\n",
    "\n",
    "- `balance`: Average yearly balance in euros (numeric)\n",
    "\n",
    "- `housing`: Has a housing loan? (categorical: \"yes\", \"no\")\n",
    "\n",
    "- `loan`: Has a personal loan? (categorical: \"yes\", \"no\")\n",
    "\n",
    "- `contact`: Type of communication contact (categorical: \"unknown\", \"telephone\", \"cellular\")\n",
    "\n",
    "- `day`: Last contact day of the month (numeric, 1-31)\n",
    "\n",
    "- `month`: Last contact month of the year (categorical: \"jan\", \"feb\", \"mar\", …, \"dec\")\n",
    "\n",
    "- `duration`: Last contact duration in seconds (numeric)\n",
    "\n",
    "- `campaign`: Number of contacts performed during this campaign (numeric)\n",
    "\n",
    "- `pdays`: Number of days since the client was last contacted from a previous campaign (numeric; -1 means the client was not previously contacted)\n",
    "\n",
    "- `previous`: Number of contacts performed before this campaign (numeric)\n",
    "\n",
    "- `poutcome`: Outcome of the previous marketing campaign (categorical: \"unknown\", \"other\", \"failure\", \"success\")\n",
    "\n",
    "- `y`: The target variable, whether the client subscribed to a term deposit (binary: \"yes\", \"no\")\n",
    "\n",
    "\n",
    "### **Observations**\n",
    "\n",
    "- No missing data.\n",
    "- Strong class imbalance (≈ 1 positive for every 8 negatives). This will require balancing strategies during training.  \n",
    "- Mix of categorical and numerical features. We need to implement encoding and scaling steps before feeding into the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f36778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical summary\n",
    "for col in categorical_features:\n",
    "    print(f\"\\nUnique values in {col}: {train[col].nunique()}\")\n",
    "    print(train[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb424fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = len(train.columns)\n",
    "ncols = 3\n",
    "nrows = int(np.ceil(len(categorical_features) / ncols))\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*5, nrows*3.5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(categorical_features):\n",
    "    vc = train[col].value_counts(normalize=True).head(TOP_K)\n",
    "    axes[i].barh(vc.index[::-1], vc.values[::-1])  # reverse to have largest on top\n",
    "    axes[i].set_title(f\"{col} (unique={train[col].nunique()})\")\n",
    "    axes[i].set_xlabel(\"Proportion\")\n",
    "    axes[i].xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.0%}\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Categorical feature proportions (top categories)\", y=1.02, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f92d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric summary\n",
    "train[numerical_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for numeric features\n",
    "ncols = 3\n",
    "nrows = int(np.ceil(len(numerical_features) / ncols))\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*5, nrows*3.5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numerical_features):\n",
    "    sns.histplot(train[col], kde=False, bins=50, ax=axes[i])\n",
    "    axes[i].set_title(f\"{col} (mean={train[col].mean():.2f}, std={train[col].std():.2f})\")\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Numeric feature histograms\", y=1.02, fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515270f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr = train[numerical_features + [target_col]].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Matrix\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f22342",
   "metadata": {},
   "source": [
    "### **Categorical Feature Analysis**\n",
    "The categorical feature proportion plots show that:\n",
    "\n",
    "- `job`: The most common occupations are *management*, *blue-collar*, and *technician*, together covering over 60% of clients. Some categories like *student* and *unknown* are rare.\n",
    "\n",
    "- `marital`: Most clients are *married* (~60%), followed by *single* and *divorced*.\n",
    "\n",
    "- `education`: The *secondary* education level dominates (~50%), followed by *tertiary* and *primary*.\n",
    "\n",
    "- `default`, `housing`, and `loan`: Almost all clients have *no default*; roughly half have *housing loans*, and most have *no personal loan*.\n",
    "\n",
    "- `contact`: The majority were contacted via *cellular*, with a small fraction through *telephone* or *unknown*.\n",
    "\n",
    "- `month`: Campaigns are concentrated in *May, August, and July*, suggesting seasonal marketing efforts.\n",
    "\n",
    "- `poutcome`: The outcome of previous campaigns is *unknown* for most clients, meaning they were not previously contacted or the outcome was not recorded.\n",
    "\n",
    "Overall, categorical data is clean and complete, though some variables are highly imbalanced (*poutcome*, *default*). These can reduce their predictive power.\n",
    "\n",
    "### **Numerical Feature Analysis**\n",
    "Numeric histograms reveal several important patterns:\n",
    "\n",
    "- `age`: Slightly right-skewed distribution centered around 40 years old.\n",
    "\n",
    "- `balance`: Highly skewed with many clients near zero balance and few with very high balances (outliers present).\n",
    "\n",
    "- `day`: Fairly uniform distribution across days of the month, suggesting no bias in call scheduling.\n",
    "\n",
    "- `duration`: Strongly right-skewed; most calls are short, but a few are very long. Longer calls correlate with positive responses (as seen in the correlation matrix).\n",
    "\n",
    "- `campaign`, `previous`, and `pdays`: Skewed towards low values, indicating most clients were contacted only once or twice, and many had never been contacted before (`pdays = 999`).\n",
    "\n",
    "### **Correlation Analysis**\n",
    "\n",
    "The correlation matrix highlights:\n",
    "\n",
    "- `duration` shows the highest positive correlation with the target variable `y` (~0.52). Longer calls tend to result in subscriptions — likely because interested clients stay on the line longer.\n",
    "\n",
    "- `previous` and `pdays` are moderately correlated (~0.56), reflecting related campaign tracking features.\n",
    "\n",
    "- Other numeric features show weak correlations, suggesting the model will benefit from non-linear combinations (perfect for an MLP).\n",
    "\n",
    "\n",
    "### **Potential Data Issues**\n",
    "| Issue | Observation | Impact | Planned Action |\n",
    "|--------|--------------|---------|----------------|\n",
    "| **Class imbalance** | Only ~12% of samples are positive (`y=1`) | May bias model towards predicting `0` | Use class-weighted loss or sampling |\n",
    "| **Outliers** | `balance` and `duration` have extreme values | Could distort scaling and gradients | Apply scaling and possibly log-transform or clip |\n",
    "| **Skewed distributions** | Most numeric features are heavily right-skewed | Normalization may not fully stabilize | Try `StandardScaler` or `RobustScaler` |\n",
    "| **Categorical imbalance** | Some categories (e.g. `unknown`, `default=yes`) are rare | Minimal contribution to learning | Consider grouping rare categories |\n",
    "| **Sentinel value** | `pdays = 999` means “never contacted before” | Misleading if treated as numeric | Add binary flag `was_contacted_before` or treat 999 as missing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccfa209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy original data\n",
    "clean_df = train.copy()\n",
    "\n",
    "# Winsorize (cap) extreme outliers for skewed features\n",
    "skewed_features = [\"balance\", \"duration\", \"campaign\", \"pdays\", \"previous\"]\n",
    "for col in skewed_features:\n",
    "    upper_limit = clean_df[col].quantile(0.99)\n",
    "    clean_df[col] = np.where(clean_df[col] > upper_limit, upper_limit, clean_df[col])\n",
    "\n",
    "print(\"Outliers capped at 99th percentile for:\", skewed_features)\n",
    "\n",
    "# Feature separation\n",
    "numeric_features = [\"age\", \"balance\", \"day\", \"duration\", \"campaign\", \"pdays\", \"previous\"]\n",
    "categorical_features = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \n",
    "                        \"loan\", \"contact\", \"month\", \"poutcome\"]\n",
    "\n",
    "X = clean_df.drop(columns=[\"id\", \"y\"])\n",
    "y = clean_df[\"y\"]\n",
    "\n",
    "# Build preprocessing pipeline\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", scaler, numeric_features),\n",
    "        (\"cat\", ohe, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit preprocessor\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(\"Transformed feature matrix shape:\", X_processed.shape)\n",
    "\n",
    "# Train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_processed, y, test_size=0.1, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape, \"Validation size:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ebefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs scaled numeric distributions\n",
    "scaled_numeric = preprocessor.named_transformers_['num'].transform(clean_df[numeric_features])\n",
    "scaled_df = pd.DataFrame(scaled_numeric, columns=numeric_features)\n",
    "\n",
    "fig, axes = plt.subplots(2, len(numeric_features)//2 + 1, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_features):\n",
    "    sns.kdeplot(clean_df[col], ax=axes[i], label=\"Before\", color=\"gray\")\n",
    "    sns.kdeplot(scaled_df[col], ax=axes[i], label=\"After\", color=\"steelblue\")\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Numeric feature scaling: Before vs After Standardization\", y=1.05, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab1ba2b",
   "metadata": {},
   "source": [
    "### **Cleaning and Preprocessing Summary**\n",
    "\n",
    "| Step | Action | Justification |\n",
    "|------|---------|----------------|\n",
    "| Missing values | None found | Dataset is complete |\n",
    "| Duplicates | Removed duplicates (if any) | Prevents bias and redundancy |\n",
    "| Outliers | Capped 99th percentile of highly skewed variables | Prevents extreme values from dominating gradients |\n",
    "| Encoding | One-Hot Encoding for categorical variables | Converts text to numeric safely for neural networks |\n",
    "| Scaling | StandardScaler (Z-score) for numeric features | Normalizes input scale for stable training |\n",
    "| Split | 90% train / 10% validation | Ensures model generalization assessment |\n",
    "\n",
    "After preprocessing:\n",
    "\n",
    "- All inputs are numeric and standardized.\n",
    "\n",
    "- The dataset is balanced across features, though the target remains imbalanced (to be handled during training).\n",
    "\n",
    "- The pipeline is saved for consistent use during testing and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d32dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure labels are numpy arrays of shape (n,)\n",
    "y_train = np.asarray(y_train).astype(np.int64).ravel()\n",
    "y_val = np.asarray(y_val).astype(np.int64).ravel()\n",
    "\n",
    "class NumPyMLP:\n",
    "    def __init__(self, input_dim, hidden_sizes=[128], lr=0.01, weight_decay=1e-4, seed=42):\n",
    "        \"\"\"\n",
    "        Simple fully-connected MLP with ReLU hidden activations and sigmoid output.\n",
    "        hidden_sizes: list of ints (e.g., [128] or [256,128]).\n",
    "        \"\"\"\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.sizes = [input_dim] + hidden_sizes + [1]  # last layer is scalar output\n",
    "        self.L = len(self.sizes) - 1  # number of weight layers\n",
    "        self.params = {}\n",
    "        # He initialization for ReLU\n",
    "        for i in range(self.L):\n",
    "            in_dim = self.sizes[i]\n",
    "            out_dim = self.sizes[i+1]\n",
    "            # weights: shape (out_dim, in_dim)\n",
    "            # use He initialization for hidden layers, small std for output\n",
    "            std = np.sqrt(2.0 / in_dim) if i < self.L - 1 else np.sqrt(1.0 / in_dim)\n",
    "            self.params[f\"W{i+1}\"] = self.rng.randn(out_dim, in_dim) * std\n",
    "            self.params[f\"b{i+1}\"] = np.zeros((out_dim, 1), dtype=np.float32)\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "    @staticmethod\n",
    "    def relu_grad(x):\n",
    "        return (x > 0).astype(np.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        # stable sigmoid\n",
    "        pos = x >= 0\n",
    "        neg = ~pos\n",
    "        out = np.empty_like(x, dtype=np.float64)\n",
    "        out[pos] = 1.0 / (1.0 + np.exp(-x[pos]))\n",
    "        ex = np.exp(x[neg])\n",
    "        out[neg] = ex / (1.0 + ex)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def bce_loss(y_true, y_pred_prob, eps=1e-12):\n",
    "        # y_true: (batch,), y_pred_prob: (batch,)\n",
    "        y_pred_prob = np.clip(y_pred_prob, eps, 1 - eps)\n",
    "        loss = - (y_true * np.log(y_pred_prob) + (1 - y_true) * np.log(1 - y_pred_prob))\n",
    "        return loss.mean()\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: shape (batch, input_dim)\n",
    "        returns: output probabilities shape (batch,), and cache for backprop\n",
    "        \"\"\"\n",
    "        cache = {}\n",
    "        A = X.T  # shape (input_dim, batch)\n",
    "        cache[\"A0\"] = A\n",
    "        for i in range(1, self.L + 1):\n",
    "            W = self.params[f\"W{i}\"]      # (out, in)\n",
    "            b = self.params[f\"b{i}\"]      # (out, 1)\n",
    "            Z = W.dot(A) + b              # (out, batch)\n",
    "            cache[f\"Z{i}\"] = Z\n",
    "            if i < self.L:\n",
    "                A = self.relu(Z)\n",
    "            else:\n",
    "                # output layer (linear -> sigmoid later)\n",
    "                A = Z\n",
    "            cache[f\"A{i}\"] = A\n",
    "        logits = cache[f\"A{self.L}\"]    # shape (1, batch)\n",
    "        probs = self.sigmoid(logits.ravel())\n",
    "        cache[\"probs\"] = probs\n",
    "        return probs, cache\n",
    "\n",
    "    def backward(self, cache, y_true):\n",
    "        \"\"\"\n",
    "        Compute gradients and return a grads dict matching params shapes.\n",
    "        y_true: shape (batch,)\n",
    "        \"\"\"\n",
    "        grads = {}\n",
    "        m = y_true.shape[0]\n",
    "        # output layer gradient\n",
    "        probs = cache[\"probs\"]         # (batch,)\n",
    "        dA = (probs - y_true) / m      # derivative of BCE wrt logits for sigmoid output\n",
    "        dA = dA.reshape(1, -1)         # (1, batch)\n",
    "        for i in range(self.L, 0, -1):\n",
    "            A_prev = cache[f\"A{i-1}\"]     # (in, batch)\n",
    "            Z_i = cache[f\"Z{i}\"]          # (out, batch)\n",
    "            W_i = self.params[f\"W{i}\"]    # (out, in)\n",
    "            # dW = dZ dot A_prev^T\n",
    "            dW = dA.dot(A_prev.T)         # (out, in)\n",
    "            db = dA.sum(axis=1, keepdims=True)  # (out,1)\n",
    "            # propagate to previous layer if not input\n",
    "            if i > 1:\n",
    "                dA_prev = W_i.T.dot(dA)          # (in, batch)\n",
    "                dZ_prev = dA_prev * self.relu_grad(cache[f\"Z{i-1}\"])  # (in, batch)\n",
    "                dA = dZ_prev\n",
    "            # include L2 regularization gradient\n",
    "            dW += self.weight_decay * W_i\n",
    "            grads[f\"dW{i}\"] = dW\n",
    "            grads[f\"db{i}\"] = db\n",
    "        return grads\n",
    "\n",
    "    def step(self, grads, lr):\n",
    "        # SGD update for each parameter\n",
    "        for i in range(1, self.L+1):\n",
    "            self.params[f\"W{i}\"] -= lr * grads[f\"dW{i}\"]\n",
    "            self.params[f\"b{i}\"] -= lr * grads[f\"db{i}\"]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        probs, _ = self.forward(X)\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        probs = self.predict_proba(X)\n",
    "        return (probs >= threshold).astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad87c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(model, X_train, y_train, X_val, y_val,\n",
    "              n_epochs=25, batch_size=1024, lr=0.01, verbose=True):\n",
    "    n_samples = X_train.shape[0]\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"val_auc\": [], \"val_acc\": []}\n",
    "    best_auc = -np.inf\n",
    "    best_params = None\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # Shuffle\n",
    "        perm = np.random.permutation(n_samples)\n",
    "        X_shuff = X_train[perm]\n",
    "        y_shuff = y_train[perm]\n",
    "        # mini-batches\n",
    "        epoch_losses = []\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            xb = X_shuff[start:start+batch_size]\n",
    "            yb = y_shuff[start:start+batch_size]\n",
    "            probs, cache = model.forward(xb)\n",
    "            loss = model.bce_loss(yb, probs)\n",
    "            epoch_losses.append(loss)\n",
    "            grads = model.backward(cache, yb)\n",
    "            model.step(grads, lr)\n",
    "\n",
    "        # epoch train loss (mean of batch losses)\n",
    "        train_loss = float(np.mean(epoch_losses))\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "\n",
    "        # validation metrics\n",
    "        val_probs, _ = model.forward(X_val)\n",
    "        val_loss = float(model.bce_loss(y_val, val_probs))\n",
    "        val_preds = (val_probs >= 0.5).astype(np.int64)\n",
    "        val_auc = roc_auc_score(y_val, val_probs)\n",
    "        val_acc = accuracy_score(y_val, val_preds)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_auc\"].append(val_auc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch:02d}/{n_epochs} — train_loss: {train_loss:.4f} — val_loss: {val_loss:.4f} — val_auc: {val_auc:.4f} — val_acc: {val_acc:.4f}\")\n",
    "\n",
    "        # checkpoint best\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            # deep copy params (simple dict copy of arrays)\n",
    "            best_params = {k: v.copy() for k, v in model.params.items()}\n",
    "\n",
    "    # restore best params\n",
    "    if best_params is not None:\n",
    "        model.params = best_params\n",
    "    print(\"Training complete. Best val AUC:\", best_auc)\n",
    "    return history\n",
    "\n",
    "# Configure and run\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_sizes = [128]          # try [256,128] for more capacity\n",
    "lr = 0.01\n",
    "batch_size = 1024\n",
    "n_epochs = 20\n",
    "weight_decay = 1e-4\n",
    "\n",
    "mlp = NumPyMLP(input_dim=input_dim, hidden_sizes=hidden_sizes, lr=lr, weight_decay=weight_decay, seed=42)\n",
    "history = train_mlp(mlp, X_train, y_train, X_val, y_val, n_epochs=n_epochs, batch_size=batch_size, lr=lr)\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"BCE Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss curves\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history[\"val_auc\"], label=\"val_auc\")\n",
    "plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.title(\"Validation AUC / Acc\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca24a3",
   "metadata": {},
   "source": [
    "### Implementation details and hyperparameter justification\n",
    "\n",
    "**Network**\n",
    "- `NumPyMLP` implements a multi-layer perceptron with general number of hidden layers.\n",
    "- Weights `W` are stored as NumPy arrays with shape `(out_dim, in_dim)` and biases `b` as `(out_dim, 1)`.\n",
    "- Hidden activations: **ReLU** (chosen for simplicity and strong empirical performance).\n",
    "- Output activation: **sigmoid**, producing probabilities for binary classification.\n",
    "\n",
    "**Initialization**\n",
    "- He initialization (`~ N(0, sqrt(2/in_dim))`) for hidden layers to help gradients flow for ReLU.\n",
    "- Output layer initialized with smaller variance.\n",
    "\n",
    "**Loss and gradients**\n",
    "- Binary cross-entropy (BCE) averaged over batch:\n",
    "  \\[\n",
    "  \\mathcal{L} = -\\frac{1}{m} \\sum_i [y_i \\log(p_i) + (1-y_i)\\log(1-p_i)]\n",
    "  \\]\n",
    "- Backpropagation computes exact gradients for all parameters using cached pre-activations.\n",
    "- L2 regularization (weight decay) added to weight gradients.\n",
    "\n",
    "**Optimizer**\n",
    "- Mini-batch SGD: chosen for simplicity and to meet the \"from-scratch\" requirement.\n",
    "- Learning rate `lr=0.01` — a typical starting point; increase/decrease depending on loss curves.\n",
    "- `weight_decay=1e-4` acts as L2 regularization to reduce overfitting.\n",
    "\n",
    "**Hyperparameters we used**\n",
    "- `hidden_sizes=[128]` — single hidden layer; you can increase units or add depths (e.g., `[256,128]`) to improve capacity.\n",
    "- `batch_size=1024` — large batch speeds up matrix ops; reduce if memory is tight.\n",
    "- `n_epochs=20` — enough to observe convergence; extend if val AUC improves.\n",
    "- `lr=0.01` — base learning rate for SGD.\n",
    "- `weight_decay=1e-4` — small L2 regularization.\n",
    "\n",
    "**Notes & next experiments**\n",
    "- Try deeper architectures or different widths if underfitting.\n",
    "- Add momentum or Adam-like updates if SGD convergence is slow (requires adding optimizer state).\n",
    "- Consider class-weighted loss or sampling if the class imbalance reduces recall for positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a0ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate on test (or validation) set\n",
    "y_true = y_val\n",
    "y_pred_proba = mlp.predict_proba(X_val)\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "auc = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "print(\"=== Test Metrics ===\")\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {auc:.4f}\")\n",
    "\n",
    "# Baseline (majority class predictor)\n",
    "majority_class = np.bincount(y_true).argmax()\n",
    "baseline_preds = np.full_like(y_true, fill_value=majority_class)\n",
    "baseline_acc = accuracy_score(y_true, baseline_preds)\n",
    "baseline_f1 = f1_score(y_true, baseline_preds, zero_division=0)\n",
    "print(\"\\n=== Baseline (Majority Class) ===\")\n",
    "print(f\"Majority class: {majority_class}\")\n",
    "print(f\"Baseline Accuracy: {baseline_acc:.4f}\")\n",
    "print(f\"Baseline F1:       {baseline_f1:.4f}\")\n",
    "\n",
    "# Table summary\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\", \"ROC-AUC\"],\n",
    "    \"MLP Model\": [acc, prec, rec, f1, auc],\n",
    "    \"Baseline\": [baseline_acc, np.nan, np.nan, baseline_f1, np.nan]\n",
    "})\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a391b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", cbar=False)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr, tpr, label=f\"MLP (AUC={auc:.3f})\")\n",
    "plt.plot([0,1], [0,1], '--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precision–Recall curve\n",
    "precisions, recalls, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(recalls, precisions, label=\"MLP\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13516ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: create submission CSV using NumPyMLP predictions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "TEST_CSV = DATA_DIR / \"test.csv\"\n",
    "SAMPLE_SUB = DATA_DIR / \"sample_submission.csv\"\n",
    "PREPROCESSOR_PATH = Path(\"artifacts\") / \"preprocessor_cleaned.joblib\"\n",
    "SUBMISSION_PATH = Path(\"submission.csv\")\n",
    "\n",
    "# 1. Load sample submission to determine format\n",
    "sample = pd.read_csv(SAMPLE_SUB)\n",
    "print(\"Sample submission columns:\", sample.columns.tolist())\n",
    "# Assume sample has columns like ['id','y'] or similar\n",
    "\n",
    "# 2. Load test data\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "if \"id\" not in test_df.columns and \"Id\" in test_df.columns:\n",
    "    test_df.rename(columns={\"Id\":\"id\"}, inplace=True)\n",
    "\n",
    "# 3. Load preprocessor and transform test features\n",
    "preprocessor = joblib.load(PREPROCESSOR_PATH)\n",
    "# Determine the feature columns the preprocessor expects.\n",
    "# If you built the pipeline with ColumnTransformer on a DataFrame, pass DataFrame slices.\n",
    "# We assume the same columns used for training: drop id and target.\n",
    "if \"id\" in test_df.columns:\n",
    "    X_test_df = test_df.drop(columns=[\"id\"])\n",
    "else:\n",
    "    X_test_df = test_df.copy()\n",
    "\n",
    "# If sample submission contains other columns, just transform test_df as we used for training\n",
    "X_test_transformed = preprocessor.transform(X_test_df)  # numpy array\n",
    "\n",
    "# 4. Predict with the NumPy MLP (mlp must be in memory)\n",
    "# If you saved model parameters in files, load them into a new NumPyMLP instance before this step.\n",
    "try:\n",
    "    probs = mlp.predict_proba(X_test_transformed)  # shape (n_samples,)\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Trained NumPyMLP instance `mlp` not found in memory. \"\n",
    "                       \"Either run training cells or load saved params into a NumPyMLP instance.\")\n",
    "\n",
    "# 5. Build submission dataframe based on sample submission columns\n",
    "# Find the name of the target column in sample_submission (commonly 'y' or 'target' or 'label')\n",
    "target_col = [c for c in sample.columns if c.lower() not in (\"id\", \"index\") and c.lower() != \"id\"][0]\n",
    "print(\"Detected target column name in sample submission:\", target_col)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "# Keep the id column exactly as in test (or as in sample submission if id is in sample)\n",
    "if \"id\" in test_df.columns:\n",
    "    submission[\"id\"] = test_df[\"id\"]\n",
    "elif \"Id\" in test_df.columns:\n",
    "    submission[\"id\"] = test_df[\"Id\"]\n",
    "else:\n",
    "    # fallback: use index\n",
    "    submission[\"id\"] = sample[\"id\"]\n",
    "\n",
    "submission[target_col] = probs  # probabilities between 0 and 1\n",
    "\n",
    "# Ensure column order matches sample\n",
    "submission = submission[sample.columns]\n",
    "\n",
    "# Save CSV (no index)\n",
    "submission.to_csv(SUBMISSION_PATH, index=False)\n",
    "print(\"Wrote submission to:\", SUBMISSION_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dabb284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
